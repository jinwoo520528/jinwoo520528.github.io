<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jinwoo Park</title>
    <meta name="author" content="Jinwoo Park" />
    <meta name="description" content="About Jinwoo Park.
" />
    <meta name="keywords" content="cv, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/al-folio/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="assets/js/theme.js"></script>
    <script src="assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="https://jinwoo520528.github.io/">about<span class="sr-only">(current)</span></a>
              </li>
              
              <li class="nav-item ">
                <a class="nav-link" href="https://drive.google.com/file/d/1d-0GHxpFnUZyqZw9uQ4vQ4dq1pzpQSM8/view?usp=sharing" target="_blank">cv</a>
              </li>

              <!-- Blog -->
              <!--li class="nav-item ">
                <a class="nav-link" href="/">blog</a>
              </li-->

              
              <!-- Other pages -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/al-folio/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/al-folio/publications/">publications</a>
              </li> -->
              <!-- <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/al-folio/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/al-folio/projects/">projects</a>
                </div>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/al-folio/teaching/">teaching</a>
              </li> -->

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Jinwoo</span> Park
          </h1>
          <p class="desc"> jinwoo520528@kaist.ac.kr | <a href="https://scholar.google.com/citations?user=C-4x6scAAAAJ&hl=en" target="_blank">Google Scholar</a> | <a href="https://www.linkedin.com/in/jinwoo520528/" target="_blank">Linkedin</a> | <a href="https://drive.google.com/file/d/1d-0GHxpFnUZyqZw9uQ4vQ4dq1pzpQSM8/view?usp=sharing" target="_blank">CV</a></p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="assets/images/profile.jpg"></source>
    <source media="(max-width: 800px)" srcset="assets/images/profile.jpg"></source>
    <source media="(max-width: 1400px)" srcset="assets/images/profile.jpg"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="assets/images/profile.jpg" alt="Jinwoo Park">

  </picture>

</figure>

            <div class="address">
              <!-- <p>555 your office number</p> <p>123 your address street</p> <p>Your City, State 12345</p> -->

            </div>
          </div>

          <div class="clearfix">
            <p>I received Ph.D. at KAIST, advised by <a href="http://ina.kaist.ac.kr/~dongsuh/" target="_blank">Dongsu Han</a> in <a href="https://ina.kaist.ac.kr/" target="_blank">INA lab</a>. 
              Before then, I received my MS degree in Electrical Engineering at KAIST, advised by <a href="https://yung-web.github.io/home/" target="_blank">Yung Yi</a>.
              Also, I have received BS degree in Electrical Engineering at KAIST. </p>
              <!-- Link to your favorite <a href="http://reddit.com" target="_blank" rel="noopener noreferrer">subreddit</a>. You can put a picture in, too. The code is already in, just name your picture <code class="language-plaintext highlighter-rouge">prof_pic.jpg</code> and put it in the <code class="language-plaintext highlighter-rouge">img/</code> folder.</p> -->

        <!-- <code class="language-plaintext highlighter-rouge">profile</code> property of the YAML header of your <code class="language-plaintext highlighter-rouge">_pages/about.md</code>. Edit <code class="language-plaintext highlighter-rouge">_bibliography/papers.bib</code> and Jekyll will render your <a href="/al-folio/publications/">publications page</a> automatically.</p> -->

<p>
  My interests are in crafting computer systems that incorporate AI/ML approaches. I concentrate on integrating AI/ML techniques into real-world systems for more efficient and better performance management policies. 
  My works aim at 1) designing efficient systems built on AI/ML approaches; 2) improving AI/ML approaches by exploiting system-specific assumptions. 
  All my work involves several months of implementation followed by thorough testing on real-world data.
  </p><p>Recently, I developed an edge-assisted inference framework that splits LLM workloads between edge and server GPU using a speculative decoding scheme. 
  In the past, I worked on various topics, including resource optimization and adaptive overload control in SLO-oriented microservices, optimizing image compression for accelerating neural restoration, and designing wireless MAC with a multi-agent RL approach.</p>
           
          <!-- </div>

          News          
          <div class="news">
            <h2>Awards and Honors</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">Feb, 2023</th>
                  <td>
                    Samsung Electronics 29th Humantech Paper Award (Silver Prize, Communication & Networks) 
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Feb, 2022</th>
                  <td>
                    Samsung Electronics PhD Scholarship <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Spring, 2021</th>
                  <td>
                    <a class="news-title" href="http://breakthroughs.kaist.ac.kr/newsletter/2021/16/default.htm" target="_blank">KAIST Breakthrough of the Year 2021, Spring (LiveNAS, NEMO)</a> 
                  </td>
                </tr> 
                <!-- <tr>
                  <th scope="row">Oct 22, 2015</th>
                  <td>
                    A simple inline announcement.
                  </td>
                </tr> 
              </table>
            </div> 
          </div> -->

          <!-- Selected papers -->
          <div class="publications">
            <h2>Selected Publications</h2>
            <ol class="bibliography"><li>
            <!-- _layouts/bib.html -->
  
            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS Spotlight</abbr></div>

              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
              
                <!-- Title -->
                <div class="title">SpecEdge: An Adaptive Top-Down Overload Control for SLO-Oriented Microservices</div>
                <!-- Author -->
                <div class="author">
                  <em>Jinwoo Park</em>, Seunggeun Cho, and Dongsu Han
                </div>

                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS),</em> 2025
                </div>
              
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://arxiv.org/abs/2505.17052" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>

                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. 
                    We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. 
                    SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. 
                    Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving.</p>
                  </div>
              </div>
            </div>
            <!-- Row start -->
            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">SIGCOMM</abbr></div>

              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
              
                <!-- Title -->
                <div class="title">TopFull: An Adaptive Top-Down Overload Control for SLO-Oriented Microservices</div>
                <!-- Author -->
                <div class="author">
                  <em>Jinwoo Park</em>, Jaehyeong Park, Youngmok Jung, Hwijoon Lim, Hyunho Yeo, and Dongsu Han
                </div>

                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>ACM Special Interest Group on Data Communication (SIGCOMM),</em> 2024
                </div>
              
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3651890.3672253" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>

                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Microservice has become a de facto standard for building large-scale cloud applications. Overload control is essential in preventing microservice failures and maintaining system performance under overloads. 
                    Although several approaches have been proposed, they are limited to mitigating overload at individual microservices lacking assessments over the interdependent microservices and APIs.

                    This paper presents TopFull, a holistic overload control framework for microservices that leverage global coordination to maximize the throughput that meet service level objectives (i.e., goodput). 
                    TopFull (a) dynamically cluster APIs according to the dependency with overloaded microservices, (b) choose APIs to load-control among those that are in contending relationships, and (c) take actions from RL agents which adaptively adjust the admitted rates of the APIs to maximize the goodput. 
                    Our experiments on various open-source benchmarks demonstrate that TopFull significantly increases the goodput under overload scenarios, outperforming DAGOR by 1.82x and Breakwater by 2.26x. 
                    Furthermore, Kubernetes autoscaler with TopFull serves up to 3.91x more requests under traffic surge and tolerates traffic spikes with up to 57% fewer resources than the Kubernetes autoscaler standalone.</p>
                </div>
              </div>
            </div>
            <!-- Row start -->
              
            <!-- Row start -->
            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">CVPR</abbr></div>
              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
                <!-- Title -->
                <div class="title">AccelIR: Task-Aware Image Compression for Accelerating Neural Restoration</div>
                <!-- Author -->
                <div class="author">
                  Juncheol Ye, Hyunho Yeo, <em>Jinwoo Park</em>, and Dongsu Han
                </div>
                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em> 2023
                </div>
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_AccelIR_Task-Aware_Image_Compression_for_Accelerating_Neural_Restoration_CVPR_2023_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>
                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Recently, deep neural networks have been successfully applied for image restoration (IR)(eg, super-resolution, de-noising, de-blurring). Despite their promising performance, running IR networks requires heavy computation. 
                    A large body of work has been devoted to addressing this issue by designing novel neural networks or pruning their parameters. 
                    However, the common limitation is that while images are saved in a compressed format before being enhanced by IR, prior work does not consider the impact of compression on the IR quality. 
                    In this paper, we present AccelIR, a framework that optimizes image compression considering the end-to-end pipeline of IR tasks. 
                    AccelIR encodes an image through IR-aware compression that optimizes compression levels across image blocks within an image according to the impact on the IR quality. 
                    Then, it runs a lightweight IR network on the compressed image, effectively reducing IR computation, while maintaining the same IR quality and image size. 
                    Our extensive evaluation using seven IR networks shows that AccelIR can reduce the computing overhead of super-resolution, de-nosing, and de-blurring by 49%, 29%, and 32% on average, respectively.</p>
                </div>
              </div>
            </div>
            <!-- Row start -->
           
            <!-- Row start -->
            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">CoNEXT</abbr></div>

              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
              
                <!-- Title -->
                <div class="title">GRAF: A graph neural network based proactive resource allocation framework for SLO-oriented microservices</div>
                <!-- Author -->
                <div class="author">
                  <em>Jinwoo Park</em>, Byungkwon Choi, Chunghan Lee, and Dongsu Han
                </div>

                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies,</em> 2021
                </div>
              
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3485983.3494866" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>

                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Microservice is an architectural style that has been widely adopted in various latency-sensitive applications. Similar to the monolith, autoscaling has attracted the attention of operators for managing resource utilization of microservices. 
                    However, it is still challenging to optimize resources in terms of latency service-level-objective (SLO) without human intervention. 
                    In this paper, we present GRAF, a graph neural network-based proactive resource allocation framework for minimizing total CPU resources while satisfying latency SLO. GRAF leverages front-end workload, distributed tracing data, and machine learning approaches to 
                    (a) observe/estimate impact of traffic change (b) find optimal resource combinations (c) make proactive resource allocation. 
                    Experiments using various open-source benchmarks demonstrate that GRAF successfully targets latency SLO while saving up to 19% of total CPU resources compared to the fine-tuned autoscaler. 
                    Moreover, GRAF handles traffic surge with 36% fewer resources while achieving up to 2.6x faster tail latency convergence compared to the Kubernetes autoscaler.</p>
                </div>
              </div>
            </div>
            <!-- Row start -->
              
            <!-- Row start -->
            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">MobiHoc</abbr></div>

              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
              
                <!-- Title -->
                <div class="title">Neuro-DCF: Design of wireless MAC via multi-agent reinforcement learning approach</div>
                <!-- Author -->
                <div class="author">
                  Sangwoo Moon, Sumyeong Ahn, Kyunghwan Son, <em>Jinwoo Park</em>, and Yung Yi
                </div>

                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing,</em> 2021
                </div>
              
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3466772.3467043" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>

                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>The carrier sense multiple access (CSMA) algorithm has been used in the wireless medium access control (MAC) under standard 802.11 implementation due to its simplicity and generality. 
                    An extensive body of research on CSMA has long been made not only in the context of practical protocols, but also in a distributed way of optimal MAC scheduling. 
                    However, the current state-of-the-art CSMA (or its extensions) still suffers from poor performance, especially in multi-hop scenarios, and often requires patch-based solutions rather than a universal solution. 
                    In this paper, we propose an algorithm which adopts an experience-driven approach and train CSMA-based wireless MAC by using deep reinforcement learning. We name our protocol, Neuro-DCF. 
                    Two key challenges are: (i) a stable training method for distributed execution and (ii) a unified training method for embracing various interference patterns and configurations. 
                    For (i), we adopt a multi-agent reinforcement learning framework, and for (ii) we introduce a novel graph neural network (GNN) based training structure. 
                    We provide extensive simulation results which demonstrate that our protocol, Neuro-DCF, significantly outperforms 802.11 DCF and O-DCF, a recent theory-based MAC protocol, especially in terms of improving delay performance while preserving optimal utility. 
                    We believe our multi-agent reinforcement learning based approach would get broad interest from other learning-based network controllers in different layers that require distributed operation.</p>
                </div>
              </div>
            </div>
            <!-- Row start -->              

          <div class="publications">
            <h2>Publications</h2>
            <ol class="bibliography"><li>

            <!-- Row start -->
                <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge">arXiv preprint</abbr></div>
                <div id="PhysRev.47.777" class="col-sm-8">
                
                  <!-- Title -->
                  <div class="title">SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs</div>
                  <!-- Author -->
                  <div class="author">
                          <em>Jinwoo Park</em>, Seunggeun Cho, and Dongsu Han
                          
                  </div>
  
                  <!-- Journal/Book title and date -->
                  <div class="periodical">
                    <em>arXiv (preprint),</em> 2025
                  </div>
                
                  <!-- Links/Buttons -->
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://arxiv.org/abs/2505.17052" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  </div>
  
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                  <p>Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. 
                    We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. 
                    SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. 
                    Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving.</p>
                  </div>
                </div>
              </div>
              
            <!-- Row start -->
                <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge">SIGCOMM</abbr></div>
                <div id="PhysRev.47.777" class="col-sm-8">
                
                  <!-- Title -->
                  <div class="title">TopFull: An Adaptive Top-Down Overload Control for SLO-Oriented Microservices</div>
                  <!-- Author -->
                  <div class="author">
                          <em>Jinwoo Park</em>, Jaehyeong Park, Youngmok Jung, Hwijoon Lim, Hyunho Yeo, and Dongsu Han
                          
                  </div>
  
                  <!-- Journal/Book title and date -->
                  <div class="periodical">
                    <em>ACM Special Interest Group on Data Communication (SIGCOMM),</em> 2024
                  </div>
                
                  <!-- Links/Buttons -->
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3651890.3672253" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  </div>
  
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                  <p>Microservice has become a de facto standard for building large-scale cloud applications. Overload control is essential in preventing microservice failures and maintaining system performance under overloads. 
                    Although several approaches have been proposed, they are limited to mitigating overload at individual microservices lacking assessments over the interdependent microservices and APIs.

                    This paper presents TopFull, a holistic overload control framework for microservices that leverage global coordination to maximize the throughput that meet service level objectives (i.e., goodput). 
                    TopFull (a) dynamically cluster APIs according to the dependency with overloaded microservices, (b) choose APIs to load-control among those that are in contending relationships, and (c) take actions from RL agents which adaptively adjust the admitted rates of the APIs to maximize the goodput. 
                    Our experiments on various open-source benchmarks demonstrate that TopFull significantly increases the goodput under overload scenarios, outperforming DAGOR by 1.82x and Breakwater by 2.26x. 
                    Furthermore, Kubernetes autoscaler with TopFull serves up to 3.91x more requests under traffic surge and tolerates traffic spikes with up to 57% fewer resources than the Kubernetes autoscaler standalone.</p>
                  </div>
                </div>
              </div>

            <!-- Row start -->
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge">ToN</abbr></div>
                <div id="PhysRev.47.777" class="col-sm-8">
                
                  <!-- Title -->
                  <div class="title">Graph Neural Network-Based SLO-Aware Proactive Resource Autoscaling Framework for Microservices</div>
                  <!-- Author -->
                  <div class="author">
                          <em>Jinwoo Park</em>, Byungkwon Choi, Chunghan Lee, and Dongsu Han
                          
                  </div>
  
                  <!-- Journal/Book title and date -->
                  <div class="periodical">
                    <em>IEEE/ACM Transactions on Networking,</em> 2024
                  </div>
                
                  <!-- Links/Buttons -->
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10518007" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  </div>
  
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>Microservice is an architectural style widely adopted in various latency-sensitive cloud applications. 
                      Similar to the monolith, autoscaling has attracted the attention of operators for managing the resource utilization of microservices. 
                      However, it is still challenging to optimize resources in terms of latency service-level-objective (SLO) without human intervention. 
                      In this paper, we present GRAF, a graph neural network-based SLO-aware proactive resource autoscaling framework for minimizing total CPU resources while satisfying latency SLO. 
                      GRAF leverages front-end workload, distributed tracing data, and machine learning approaches to (a) observe/estimate the impact of traffic change (b) find optimal resource combinations (c) make proactive resource allocation. 
                      Experiments using various open-source benchmarks demonstrate that GRAF successfully targets latency SLO while saving up to 19% of total CPU resources compared to the fine-tuned autoscaler. 
                      GRAF also handles a traffic surge with 36% fewer resources while achieving up to 2.6x faster tail latency convergence compared to the Kubernetes autoscaler. 
                      Moreover, we verify the scalability of GRAF on large-scale deployments, where GRAF saves 21.6% and 25.4% for CPU resources and memory resources, respectively.</p>
                  </div>
                </div>
              </div>

            <!-- Row start -->

            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">HotStorage</abbr></div>

              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
              
                <!-- Title -->
                <div class="title">SAND: A Storage Abstraction for Video-based Deep Learning</div>
                <!-- Author -->
                <div class="author">
                        Uitaek Hong, Hwijoon Lim, Hyunho Yeo, <em>Jinwoo, Park</em>, and Dongsu Han
                        
                </div>

                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>15th ACM Workshop on Hot Topics in Storage and File Systems,</em> 2023
                </div>
              
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3599691.3603407" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>

                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>To  Appear.</p>
                </div>
              </div>
            </div>
              
              <div class="row">

                  <div class="col-sm-2 abbr"><abbr class="badge">CVPR</abbr></div>
    
                  <!-- Entry bib key -->
                  <div id="PhysRev.47.777" class="col-sm-8">
                  
                    <!-- Title -->
                    <div class="title">AccelIR: Task-Aware Image Compression for Accelerating Neural Restoration</div>
                    <!-- Author -->
                    <div class="author">
                            Juncheol Ye, Hyunho Yeo <em>Jinwoo, Park</em>, and Dongsu Han
                            
                    </div>
    
                    <!-- Journal/Book title and date -->
                    <div class="periodical">
                      <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em> 2023
                    </div>
                  
                    <!-- Links/Buttons -->
                    <div class="links">
                      <a class="abstract btn btn-sm z-depth-0" role="button">ABS</a>
                      <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_AccelIR_Task-Aware_Image_Compression_for_Accelerating_Neural_Restoration_CVPR_2023_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                    </div>
    
                    <!-- Hidden abstract block -->
                    <div class="abstract hidden">
                      <p>Recently, deep neural networks have been successfully applied for image restoration (IR)(eg, super-resolution, de-noising, de-blurring). Despite their promising performance, running IR networks requires heavy computation. 
                        A large body of work has been devoted to addressing this issue by designing novel neural networks or pruning their parameters. 
                        However, the common limitation is that while images are saved in a compressed format before being enhanced by IR, prior work does not consider the impact of compression on the IR quality. 
                        In this paper, we present AccelIR, a framework that optimizes image compression considering the end-to-end pipeline of IR tasks. 
                        AccelIR encodes an image through IR-aware compression that optimizes compression levels across image blocks within an image according to the impact on the IR quality. 
                        Then, it runs a lightweight IR network on the compressed image, effectively reducing IR computation, while maintaining the same IR quality and image size. 
                        Our extensive evaluation using seven IR networks shows that AccelIR can reduce the computing overhead of super-resolution, de-nosing, and de-blurring by 49%, 29%, and 32% on average, respectively.</p>                    
                    </div>
                  </div>
                </div>


              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge">CoNEXT</abbr></div>
                <div id="PhysRev.47.777" class="col-sm-8">
                
                  <!-- Title -->
                  <div class="title">GRAF: A graph neural network based proactive resource allocation framework for SLO-oriented microservices</div>
                  <!-- Author -->
                  <div class="author">
                          <em>Jinwoo Park</em>, Byungkwon Choi, Chunghan Lee, and Dongsu Han
                          
                  </div>
  
                  <!-- Journal/Book title and date -->
                  <div class="periodical">
                    <em>Proceedings of the 17th International Conference on emerging Networking EXperiments and Technologies,</em> 2021
                  </div>
                
                  <!-- Links/Buttons -->
                  <div class="links">
                    <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3485983.3494866" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  </div>
  
                  <!-- Hidden abstract block -->
                  <div class="abstract hidden">
                    <p>Microservice is an architectural style that has been widely adopted in various latency-sensitive applications. Similar to the monolith, autoscaling has attracted the attention of operators for managing resource utilization of microservices. 
                      However, it is still challenging to optimize resources in terms of latency service-level-objective (SLO) without human intervention. 
                      In this paper, we present GRAF, a graph neural network-based proactive resource allocation framework for minimizing total CPU resources while satisfying latency SLO. GRAF leverages front-end workload, distributed tracing data, and machine learning approaches to 
                      (a) observe/estimate impact of traffic change (b) find optimal resource combinations (c) make proactive resource allocation. 
                      Experiments using various open-source benchmarks demonstrate that GRAF successfully targets latency SLO while saving up to 19% of total CPU resources compared to the fine-tuned autoscaler. 
                      Moreover, GRAF handles traffic surge with 36% fewer resources while achieving up to 2.6x faster tail latency convergence compared to the Kubernetes autoscaler.</p>
                  </div>
                </div>
              </div>

            <!-- Row start -->
            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">MobiHoc</abbr></div>

              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
              
                <!-- Title -->
                <div class="title">Neuro-DCF: Design of wireless MAC via multi-agent reinforcement learning approach</div>
                <!-- Author -->
                <div class="author">
                        Sangwoo Moon, Sumyeong Ahn, Kyunghwan Son, <em>Jinwoo Park</em>, and Yung Yi
                        
                </div>

                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>Proceedings of the Twenty-second International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing,</em> 2021
                </div>
              
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3466772.3467043" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>

                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>The carrier sense multiple access (CSMA) algorithm has been used in the wireless medium access control (MAC) under standard 802.11 implementation due to its simplicity and generality. 
                    An extensive body of research on CSMA has long been made not only in the context of practical protocols, but also in a distributed way of optimal MAC scheduling. 
                    However, the current state-of-the-art CSMA (or its extensions) still suffers from poor performance, especially in multi-hop scenarios, and often requires patch-based solutions rather than a universal solution. 
                    In this paper, we propose an algorithm which adopts an experience-driven approach and train CSMA-based wireless MAC by using deep reinforcement learning. We name our protocol, Neuro-DCF. 
                    Two key challenges are: (i) a stable training method for distributed execution and (ii) a unified training method for embracing various interference patterns and configurations. 
                    For (i), we adopt a multi-agent reinforcement learning framework, and for (ii) we introduce a novel graph neural network (GNN) based training structure. 
                    We provide extensive simulation results which demonstrate that our protocol, Neuro-DCF, significantly outperforms 802.11 DCF and O-DCF, a recent theory-based MAC protocol, especially in terms of improving delay performance while preserving optimal utility. 
                    We believe our multi-agent reinforcement learning based approach would get broad interest from other learning-based network controllers in different layers that require distributed operation.</p>                
                </div>
              </div>
            </div>
            <!-- Row start -->
            <div class="row">
              <div class="col-sm-2 abbr"><abbr class="badge">APNet</abbr></div>

              <!-- Entry bib key -->
              <div id="PhysRev.47.777" class="col-sm-8">
              
                <!-- Title -->
                <div class="title">pHPA: A proactive autoscaling framework for microservice chain</div>
                <!-- Author -->
                <div class="author">
                        Byungkwon Choi, <em>Jinwoo Park</em>, Chunghan Lee, and Dongsu Han
                        
                </div>

                <!-- Journal/Book title and date -->
                <div class="periodical">
                  <em>5th Asia-Pacific Workshop on Networking,</em> 2021
                </div>
              
                <!-- Links/Buttons -->
                <div class="links">
                  <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3469393.3469401" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
                  <!-- <a href="assets/pdf/example_pdf.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> -->
                </div>

                <!-- Hidden abstract block -->
                <div class="abstract hidden">
                  <p>Microservice is an architectural style that breaks down monolithic applications into smaller microservices and has been widely adopted by a variety of enterprises. 
                    Like the monolith, autoscaling has attracted the attention of operators in scaling microservices. However, most existing approaches of autoscaling do not consider microservice chain and severely degrade the performance of microservices when traffic surges. 
                    In this paper, we present pHPA, an autoscaling framework for the microservice chain. pHPA proactively allocates resources to the microservice chains and effectively handles traffic surges. 
                    Our evaluation using various open-source benchmarks shows that pHPA reduces 99%-tile latency and resource usage by up to 70% and 58% respectively compared to the most widely used autoscaler when traffic surges.</p>
                </div>
              </div>
            </div>

            </li></ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <!-- <a href="mailto:jinwoo520528@kaist.ac.kr" title="email"><i class="fas fa-envelope"></i></a> -->
            <!-- <a href="http://localhost:4000/al-folio/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> -->
            
            </div>

            <div class="contact-note">
              <!-- Contact -->

            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        <!-- © Copyright 2022 Youngmok Jung. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. -->

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="assets/js/common.js"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DCDCK3RW8C"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DCDCK3RW8C');
</script>
    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
